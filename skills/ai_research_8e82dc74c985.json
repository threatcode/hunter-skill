{
  "id": "ai_research_8e82dc74c985",
  "category": "ai-research",
  "title": "monitoring",
  "description": "# AI monitoring tools\n\n1. **Model Monitoring Tools**\n   - [MLflow](https://mlflow.org/)\n   - [TensorFlow Extended (TFX)](https://www.tensorflow.org/tfx)\n   - [Seldon](https://www.seldon.io/)\n\n2. **Data Quality Tools**\n   - [Great Expectations](https://greatexpectations.io/)\n   - [Deequ](https://github.com/awslabs/deequ)\n\n3. **Explainability and Interpretability Tools**\n   - [SHAP (SHapley Additive exPlanations)](https://shap.readthedocs.io/en/latest/)\n   - [LIME (Local Interpretable Model-agnost",
  "payloads": [
    "# AI monitoring tools",
    "1. **Model Monitoring Tools**",
    "- [MLflow](https://mlflow.org/)",
    "- [TensorFlow Extended (TFX)](https://www.tensorflow.org/tfx)",
    "- [Seldon](https://www.seldon.io/)",
    "2. **Data Quality Tools**",
    "- [Great Expectations](https://greatexpectations.io/)",
    "- [Deequ](https://github.com/awslabs/deequ)",
    "3. **Explainability and Interpretability Tools**",
    "- [SHAP (SHapley Additive exPlanations)](https://shap.readthedocs.io/en/latest/)",
    "- [LIME (Local Interpretable Model-agnostic Explanations)](https://github.com/marcotcr/lime)",
    "4. **Ethical and Bias Monitoring Tools**",
    "- [IBM's AI Fairness 360](https://www.ibm.com/opensource/open/projects/ai-fairness-360/)",
    "- [Google's What-If Tool](https://pair-code.github.io/what-if-tool/)",
    "5. **Performance Monitoring Tools**"
  ],
  "source": "h4cker",
  "references": [
    "/workspaces/hunter-skill/h4cker/ai-research/monitoring.md"
  ]
}