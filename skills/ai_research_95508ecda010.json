{
  "id": "ai_research_95508ecda010",
  "category": "ai-research",
  "title": "model security testing",
  "description": "Please see https://github.com/The-Art-of-Hacking/h4cker/blob/master/ai-research/ai_security_tools.md\n\n### Julia Adversarial ML Frameworks\n\n- **[Mirage](https://github.com/bad-antics/mirage)** - Adversarial machine learning framework in Julia. Implements evasion attacks (FGSM, PGD, C&W), model extraction, membership inference, and robustness testing. Includes defenses like adversarial training and certified robustness. 7,000+ lines of Julia code.\n",
  "payloads": [
    "Please see https://github.com/The-Art-of-Hacking/h4cker/blob/master/ai-research/ai_security_tools.md",
    "### Julia Adversarial ML Frameworks",
    "- **[Mirage](https://github.com/bad-antics/mirage)** - Adversarial machine learning framework in Julia. Implements evasion attacks (FGSM, PGD, C&W), model extraction, membership inference, and robustness testing. Includes defenses like adversarial training and certified robustness. 7,000+ lines of Julia code."
  ],
  "source": "h4cker",
  "references": [
    "/workspaces/hunter-skill/h4cker/ai-research/model_security_testing.md"
  ]
}