{
  "id": "ai_c17b90d48828",
  "category": "AI",
  "title": "AI Risk Frameworks",
  "description": "# AI Risks\n\n{{#include ../banners/hacktricks-training.md}}\n\n## OWASP Top 10 Machine Learning Vulnerabilities\n\nOwasp has identified the top 10 machine learning vulnerabilities that can affect AI systems. These vulnerabilities can lead to various security issues, including data poisoning, model inversion, and adversarial attacks. Understanding these vulnerabilities is crucial for building secure AI systems.\n\nFor an updated and detailed list of the top 10 machine learning vulnerabilities, refer to ",
  "payloads": [
    "# AI Risks",
    "{{#include ../banners/hacktricks-training.md}}",
    "## OWASP Top 10 Machine Learning Vulnerabilities",
    "Owasp has identified the top 10 machine learning vulnerabilities that can affect AI systems. These vulnerabilities can lead to various security issues, including data poisoning, model inversion, and adversarial attacks. Understanding these vulnerabilities is crucial for building secure AI systems.",
    "For an updated and detailed list of the top 10 machine learning vulnerabilities, refer to the [OWASP Top 10 Machine Learning Vulnerabilities](https://owasp.org/www-project-machine-learning-security-top-10/) project.",
    "- **Input Manipulation Attack**: An attacker adds tiny, often invisible changes to **incoming data** so the model makes the wrong decision.\\",
    "*Example*: A few specks of paint on a stop\u2011sign fool a self\u2011driving car into \"seeing\" a speed\u2011limit sign.",
    "- **Data Poisoning Attack**: The **training set** is deliberately polluted with bad samples, teaching the model harmful rules.\\",
    "*Example*: Malware binaries are mislabeled as \"benign\" in an antivirus training corpus, letting similar malware slip past later.",
    "- **Model Inversion Attack**: By probing outputs, an attacker builds a **reverse model** that reconstructs sensitive features of the original inputs.\\",
    "*Example*: Re\u2011creating a patient's MRI image from a cancer\u2011detection model's predictions.",
    "- **Membership Inference Attack**: The adversary tests whether a **specific record** was used during training by spotting confidence differences.\\",
    "*Example*: Confirming that a person's bank transaction appears in a fraud\u2011detection model's training data.",
    "- **Model Theft**: Repeated querying lets an attacker learn decision boundaries and **clone the model's behavior** (and IP).\\",
    "*Example*: Harvesting enough Q&A pairs from an ML\u2011as\u2011a\u2011Service API to build a near\u2011equivalent local model."
  ],
  "source": "HackTricks",
  "references": [
    "/workspaces/hunter-skill/hacktricks/src/AI/AI-Risk-Frameworks.md"
  ]
}